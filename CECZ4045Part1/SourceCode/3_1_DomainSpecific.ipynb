{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 8517,
     "status": "ok",
     "timestamp": 1602422420884,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "MWzBiF5ooV3g",
    "outputId": "3c03452f-c37b-4e99-ff31-3384436e4265"
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import PorterStemmer\n",
    "porter=PorterStemmer()\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
    "!pip install -q praw\n",
    "import praw\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import random\n",
    "reddit = praw.Reddit(client_id='H_TkAcrhaSZH4g',\n",
    "                     client_secret='qD3XYQe5fmWxDlHCT-8f36Iqd6c',\n",
    "                     user_agent='tlim045')\n",
    "headlines = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PrQeRNrwOHK"
   },
   "outputs": [],
   "source": [
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele\n",
    "        str1 += ' '   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10095,
     "status": "ok",
     "timestamp": 1602231057064,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "czjBm0StmeKx",
    "outputId": "6b854702-d7a1-4c86-c996-081f78fdab08"
   },
   "outputs": [],
   "source": [
    "print(\"r/wallstreetbets\")\n",
    "wsb_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a'] # TODO: remove profanities here\n",
    "wsb_chosen_comments = []\n",
    "wsb_token_length = []\n",
    "wsb_sentence_length = []\n",
    "\n",
    "count = 0\n",
    "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "  for comment in submission.comments.list():\n",
    "    if(comment.author != \"WSBVoteBot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count>=20):\n",
    "        break\n",
    "      else:\n",
    "        w = nltk.word_tokenize(comment.body)\n",
    "        wsb_words.extend(w)\n",
    "        count+=1\n",
    "        print(\"Comment:\", comment.body)\n",
    "        print(w)\n",
    "\n",
    "        print(\"After Stemming: \")\n",
    "        # stem and lower each word and remove duplicates\n",
    "        wsb_words_stem = [porter.stem(p.lower()) for p in w if p not in ignore_words]\n",
    "        wsb_words_stem = sorted(list(set(wsb_words_stem)))\n",
    "\n",
    "        print(\"length of a token in number of characters\")\n",
    "        for i in wsb_words_stem:\n",
    "          print(len(i))\n",
    "          wsb_token_length.append(len(i))\n",
    "        print(wsb_words_stem)  \n",
    "        print(sent_tokenize(comment.body))\n",
    "        print(\"Number of sentences: \", len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment\n",
    "        wsb_sentence_length.append(len(sent_tokenize(comment.body)))\n",
    "count = 0\n",
    "listofcomments = []\n",
    "\n",
    "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "  listofcomments+=submission.comments.list()\n",
    "while(True):\n",
    "  chosen_comment = random.choice(listofcomments)\n",
    "  if(chosen_comment.author != \"WSBVoteBot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count <= 2):\n",
    "        count+=1\n",
    "        w = nltk.word_tokenize(chosen_comment.body)\n",
    "        print(nltk.pos_tag(w))\n",
    "      else:\n",
    "        break\n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9975,
     "status": "ok",
     "timestamp": 1602235608321,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "3hX4wOKnQG3e",
    "outputId": "f7c1965b-2d72-4c71-8fd4-adb6692aeb57"
   },
   "outputs": [],
   "source": [
    "print(\"r/mechanicalkeyboards\")\n",
    "mechanical_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a']\n",
    "mechanical_token_length = []\n",
    "mechanical_sentence_length = []\n",
    "count = 0\n",
    "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=10):\n",
    "    for comment in submission.comments.list():\n",
    "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "        if(count>=20):\n",
    "          break\n",
    "        else:\n",
    "          s = nltk.word_tokenize(comment.body)\n",
    "          mechanical_words.extend(s)\n",
    "          count+=1\n",
    "          print(\"Comment:\", comment.body)\n",
    "          print(s)\n",
    "\n",
    "          print(\"After Stemming: \")\n",
    "          # stem and lower each word and remove duplicates\n",
    "          mechanical_words_stem = [porter.stem(p.lower()) for p in s if p not in ignore_words]\n",
    "          mechanical_words_stem = sorted(list(set(mechanical_words_stem)))\n",
    "          print(\"length of a token in number of characters\")\n",
    "          for i in mechanical_words_stem:\n",
    "            print(len(i))\n",
    "            mechanical_token_length.append(len(i))\n",
    "          print(mechanical_words_stem)     \n",
    "          print(\"Number of sentences:\" , len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment \n",
    "          mechanical_sentence_length.append(len(sent_tokenize(comment.body)))\n",
    "\n",
    "count = 0\n",
    "listofcomments = []\n",
    "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=10):\n",
    "  listofcomments+=submission.comments.list()\n",
    "while(True):\n",
    "  chosen_comment = random.choice(listofcomments)\n",
    "  if(chosen_comment.author != \"sneakpeek_bot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and chosen_comment.author != \"Reddit-Book-Bot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count <= 2):\n",
    "        count+=1\n",
    "        w = nltk.word_tokenize(chosen_comment.body)\n",
    "        print(nltk.pos_tag(w))\n",
    "      else:\n",
    "        break   \n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 26048,
     "status": "ok",
     "timestamp": 1602234285886,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "ofZ5JNI2C2_1",
    "outputId": "bf7cd41b-5ebe-4543-f4dd-9a6770e1f21a"
   },
   "outputs": [],
   "source": [
    "print(\"r/programming\")\n",
    "program_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a']\n",
    "program_token_length = []\n",
    "program_sentence_length = []\n",
    "program_sentence = []\n",
    "count = 0\n",
    "for submission in reddit.subreddit('programming').new(limit=30):\n",
    "    for comment in submission.comments.list():\n",
    "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\"): \n",
    "        if(count>=20):\n",
    "          break\n",
    "        else:\n",
    "          p = nltk.word_tokenize(comment.body)\n",
    "          program_words.extend(p)\n",
    "          count+=1\n",
    "          print(\"Comment:\", comment.body)\n",
    "          print(p)\n",
    "\n",
    "          print(\"After Stemming: \")\n",
    "          # stem and lower each word and remove duplicates\n",
    "          program_words_stem = [porter.stem(x.lower()) for x in p if x not in ignore_words]\n",
    "          program_words_stem = sorted(list(set(program_words_stem)))\n",
    "          print(\"length of a token in number of characters\")\n",
    "          for i in program_words_stem:\n",
    "            print(len(i))\n",
    "            program_token_length.append(len(i))\n",
    "          print(program_words_stem)\n",
    "          print(\"Number of sentences: \" , len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment \n",
    "          program_sentence_length.append(len(sent_tokenize(comment.body)))   \n",
    "count = 0\n",
    "for submission in reddit.subreddit('programming').new(limit=30):\n",
    "  listofcomments+=submission.comments.list()\n",
    "while(True):\n",
    "  chosen_comment = random.choice(listofcomments)\n",
    "  if(chosen_comment.author != \"sneakpeek_bot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and chosen_comment.author != \"Reddit-Book-Bot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count <= 2):\n",
    "        count+=1\n",
    "        w = nltk.word_tokenize(chosen_comment.body)\n",
    "        print(nltk.pos_tag(w))\n",
    "      else:\n",
    "        break \n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNNfaUPD0kep"
   },
   "outputs": [],
   "source": [
    "#Dataframe set up\n",
    "wsb_df = pd.DataFrame(wsb_token_length, columns = ['Token length'])\n",
    "mechanical_df = pd.DataFrame(mechanical_token_length, columns = ['Token length'])\n",
    "program_df = pd.DataFrame(program_token_length, columns = ['Token length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1602231216664,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "nSMq-cy_4o7f",
    "outputId": "178091da-be39-480c-8a29-979a2c5c0434"
   },
   "outputs": [],
   "source": [
    "wsb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1602231219190,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "v4ymblsT4rqz",
    "outputId": "34cdada2-aea3-4323-ceff-6b6b3bf46181"
   },
   "outputs": [],
   "source": [
    "mechanical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 936,
     "status": "ok",
     "timestamp": 1602231220297,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "VUanTFqr4tbq",
    "outputId": "7dc066ea-af76-426d-e6a6-b483cf87635b"
   },
   "outputs": [],
   "source": [
    "program_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DL360y5t3hah"
   },
   "outputs": [],
   "source": [
    "wsb_df = wsb_df.groupby([\"Token length\"])[\"Token length\"].count().reset_index(name=\"count\")\n",
    "\n",
    "mechanical_df = mechanical_df.groupby([\"Token length\"])[\"Token length\"].count().reset_index(name=\"count\")\n",
    "\n",
    "program_df = program_df.groupby([\"Token length\"])[\"Token length\"].count().reset_index(name=\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "executionInfo": {
     "elapsed": 957,
     "status": "ok",
     "timestamp": 1602231543391,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "9XArDHv64kqB",
    "outputId": "25add138-6414-4c68-8b7a-a8cad928861f"
   },
   "outputs": [],
   "source": [
    "wsb_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 976,
     "status": "ok",
     "timestamp": 1602231822185,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "o0NWFTpucTH1",
    "outputId": "6c087f29-f371-43f6-a8f6-4231d5d40d57"
   },
   "outputs": [],
   "source": [
    "wsb_df.plot(kind = 'bar', x = 'Token length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1602231248352,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "coJinrHQbIw0",
    "outputId": "bd5cf3d3-c1c2-4b16-f8f9-750857fa525a"
   },
   "outputs": [],
   "source": [
    "mechanical_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1030,
     "status": "ok",
     "timestamp": 1602231898847,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "bqHFSLKUdm0Q",
    "outputId": "e0c500ad-81ee-4a3b-8433-7b59744a9986"
   },
   "outputs": [],
   "source": [
    "mechanical_df.plot(kind = 'bar', x = 'Token length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 607
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1602231256838,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "Ejf7HiwWbLBl",
    "outputId": "3a5b4b64-2be8-49dd-f12a-9076d67f4811"
   },
   "outputs": [],
   "source": [
    "program_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1049,
     "status": "ok",
     "timestamp": 1602231922415,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "LK8MEIkmdrAm",
    "outputId": "58599ba4-21be-4bac-f4fa-a55995951e66"
   },
   "outputs": [],
   "source": [
    "program_df.plot(kind = 'bar', x = 'Token length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8OPv6ewdzaG"
   },
   "outputs": [],
   "source": [
    "wsb_sentence_df = pd.DataFrame(wsb_sentence_length, columns = ['Sentence length'])\n",
    "mechanical_sentence_df = pd.DataFrame(mechanical_sentence_length, columns = ['Sentence length'])\n",
    "program_sentence_df = pd.DataFrame(program_sentence_length, columns = ['Sentence length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6WhvhAmeoha"
   },
   "outputs": [],
   "source": [
    "wsb_sentence_df = wsb_sentence_df.groupby([\"Sentence length\"])[\"Sentence length\"].count().reset_index(name=\"count\")\n",
    "\n",
    "mechanical_sentence_df = mechanical_sentence_df.groupby([\"Sentence length\"])[\"Sentence length\"].count().reset_index(name=\"count\")\n",
    "\n",
    "program_sentence_df = program_sentence_df.groupby([\"Sentence length\"])[\"Sentence length\"].count().reset_index(name=\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 2415,
     "status": "ok",
     "timestamp": 1602346791166,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "1dZFGYIlfMKU",
    "outputId": "36302269-589a-4f65-8706-538ecc011af5"
   },
   "outputs": [],
   "source": [
    "wsb_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1955,
     "status": "ok",
     "timestamp": 1602346795200,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "AHtdOzuwfYT9",
    "outputId": "772513b2-92be-40a5-cb5b-9949ee0cec86"
   },
   "outputs": [],
   "source": [
    "wsb_sentence_df.plot(kind = 'bar', x = 'Sentence length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 491
    },
    "executionInfo": {
     "elapsed": 1984,
     "status": "ok",
     "timestamp": 1602346797609,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "zr3OiLfHfOcA",
    "outputId": "62159c9c-9a25-4e7e-9915-d210824b351b"
   },
   "outputs": [],
   "source": [
    "mechanical_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1643,
     "status": "ok",
     "timestamp": 1602346798896,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "WL-ipo96fdCF",
    "outputId": "5c31c2f5-1c76-4cae-8648-957b39e20b2d"
   },
   "outputs": [],
   "source": [
    "mechanical_sentence_df.plot(kind = 'bar', x = 'Sentence length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 698
    },
    "executionInfo": {
     "elapsed": 1310,
     "status": "ok",
     "timestamp": 1602346802255,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "AnST4VWdfRcO",
    "outputId": "745ab319-7056-43c9-8fd4-6bade1517ac4"
   },
   "outputs": [],
   "source": [
    "program_sentence_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1602346804019,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "ZM06dJ-7fe0i",
    "outputId": "01b648ee-9d52-43f9-dedf-5b341b94e779"
   },
   "outputs": [],
   "source": [
    "program_sentence_df.plot(kind = 'bar', x = 'Sentence length', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 6540,
     "status": "ok",
     "timestamp": 1602335524686,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "itPUVNhn3XdL",
    "outputId": "1af3a1e2-e116-4741-b626-fe8478691714"
   },
   "outputs": [],
   "source": [
    "print(\"r/wallstreetbets\")\n",
    "wsb_words1 = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a'] # TODO: remove profanities here\n",
    "wsb_chosen_comments = []\n",
    "wsb_token_length_before = []\n",
    "wsb_token_length_after = []\n",
    "wsb_sentence_length1 = []\n",
    "\n",
    "count = 0\n",
    "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "  for comment in submission.comments.list():\n",
    "    if(comment.author != \"WSBVoteBot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count>=20):\n",
    "        break\n",
    "      else:\n",
    "        w = nltk.word_tokenize(comment.body)\n",
    "        for i in w:\n",
    "          wsb_token_length_before.append(len(i))\n",
    "        wsb_words1.extend(w)\n",
    "        count+=1\n",
    "        print(\"Comment:\", comment.body)\n",
    "        print(w)\n",
    "\n",
    "        print(\"After Stemming: \")\n",
    "        # stem and lower each word and remove duplicates\n",
    "        wsb_words_stem = [porter.stem(p.lower()) for p in w if p not in ignore_words]\n",
    "        wsb_words_stem = sorted(list(set(wsb_words_stem)))\n",
    "\n",
    "        print(\"length of a token in number of characters\")\n",
    "        for i in wsb_words_stem:\n",
    "          wsb_token_length_after.append(len(i))\n",
    "        print(wsb_words_stem)  \n",
    "        print(sent_tokenize(comment.body))\n",
    "        print(\"Number of sentences: \", len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment\n",
    "        wsb_sentence_length1.append(len(sent_tokenize(comment.body)))\n",
    "count = 0\n",
    "listofcomments1=[]\n",
    "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "  listofcomments1+=submission.comments.list()\n",
    "while(True):\n",
    "  chosen_comment = random.choice(listofcomments1)\n",
    "  if(chosen_comment.author != \"WSBVoteBot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count <= 2):\n",
    "        count+=1\n",
    "        w = nltk.word_tokenize(chosen_comment.body)\n",
    "        print(nltk.pos_tag(w))\n",
    "      else:\n",
    "        break\n",
    "  else:\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEmqFaYzpB6a"
   },
   "outputs": [],
   "source": [
    "wsb_df_before = pd.DataFrame(wsb_token_length_before, columns = ['Token length before'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1602335677913,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "KUV1ZU_ypKBC",
    "outputId": "78867c74-022b-4998-f081-46484256f918"
   },
   "outputs": [],
   "source": [
    "wsb_df_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XWNARmXpnjR"
   },
   "outputs": [],
   "source": [
    "wsb_df_before = wsb_df_before.groupby([\"Token length before\"])[\"Token length before\"].count().reset_index(name=\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "executionInfo": {
     "elapsed": 763,
     "status": "ok",
     "timestamp": 1602335785160,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "xZOJpxtxp6vH",
    "outputId": "8ee5a83a-e726-4351-b9c8-e40dafe3721f"
   },
   "outputs": [],
   "source": [
    "wsb_df_before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 1262,
     "status": "ok",
     "timestamp": 1602335823984,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "r9DKqx0bqBeH",
    "outputId": "26452522-bf42-4ae4-cb5a-0834300abba8"
   },
   "outputs": [],
   "source": [
    "wsb_df_before.plot(kind = 'bar', x = 'Token length before', y = 'count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFKEsXnap72c"
   },
   "outputs": [],
   "source": [
    "wsb_after_df = pd.DataFrame(wsb_token_length_after, columns = ['Token length after'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mNerFgiAtTaR"
   },
   "outputs": [],
   "source": [
    "wsb_after_df = wsb_after_df.groupby([\"Token length after\"])[\"Token length after\"].count().reset_index(name=\"count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 321
    },
    "executionInfo": {
     "elapsed": 824,
     "status": "ok",
     "timestamp": 1602336702109,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "6x4V7pDxtYaX",
    "outputId": "47fc4c13-db4f-46dc-8d89-b9ba7a1826d5"
   },
   "outputs": [],
   "source": [
    "wsb_after_df.plot(kind = 'bar', x = 'Token length after', y = 'count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aeMsnyKqEIw1"
   },
   "source": [
    "For Sentence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3118,
     "status": "ok",
     "timestamp": 1602343548566,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "MTgUc6fIEKzw",
    "outputId": "8198ed10-027f-4a61-f934-71fc25f511f9"
   },
   "outputs": [],
   "source": [
    "print(\"r/wallstreetbets\")\n",
    "wsb_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a'] # TODO: remove profanities here\n",
    "wsb_chosen_comments = []\n",
    "wsb_token_length = []\n",
    "wsb_number_sentence = []\n",
    "wsb_sentence_length = []\n",
    "wsb_sentence = []\n",
    "\n",
    "count = 0\n",
    "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "  for comment in submission.comments.list():\n",
    "    if(comment.author != \"WSBVoteBot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "      if(count>=20):\n",
    "        break\n",
    "      else:\n",
    "        w = nltk.word_tokenize(comment.body)\n",
    "        wsb_words.extend(w)\n",
    "        count+=1\n",
    "        print(\"Comment:\", comment.body)\n",
    "        print(w)\n",
    "\n",
    "        print(\"After Stemming: \")\n",
    "        # stem and lower each word and remove duplicates\n",
    "        wsb_words_stem = [porter.stem(p.lower()) for p in w if p not in ignore_words]\n",
    "        wsb_words_stem = sorted(list(set(wsb_words_stem)))\n",
    "\n",
    "        print(\"length of a token in number of characters\")\n",
    "        for i in wsb_words_stem:\n",
    "          print(len(i))\n",
    "          wsb_token_length.append(len(i))\n",
    "        print(\"before wsb_words_stem\")\n",
    "        wsb_sentence.extend(sent_tokenize(comment.body))\n",
    "        print(sent_tokenize(comment.body))\n",
    "        print(\"Number of sentences: \", len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment\n",
    "        wsb_number_sentence.append(len(sent_tokenize(comment.body)))\n",
    "print(wsb_sentence)\n",
    "for i in wsb_sentence:\n",
    "  wsb_sentence_length.append(len(i.split()))\n",
    "\n",
    "\n",
    "#for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
    "#  listofcomments+=submission.comments.list()\n",
    "#while(True):\n",
    "#  chosen_comment = random.choice(listofcomments)\n",
    "#  if(chosen_comment.author != \"WSBVoteBot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "#      if(count <= 2):\n",
    "#        count+=1\n",
    "#        w = nltk.word_tokenize(chosen_comment.body)\n",
    "#        print(nltk.pos_tag(w))\n",
    "#      else:\n",
    "#        break\n",
    "#  else:\n",
    "#    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2152,
     "status": "ok",
     "timestamp": 1602343765863,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "k687GRLhHquU",
    "outputId": "c38b6365-18fb-4587-f733-5aa9302d6e14"
   },
   "outputs": [],
   "source": [
    "print(\"r/mechanicalkeyboards\")\n",
    "mechanical_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a']\n",
    "mechanical_token_length = []\n",
    "mechanical_sentence_length = []\n",
    "mechanical_sentence = []\n",
    "count = 0\n",
    "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=10):\n",
    "    for comment in submission.comments.list():\n",
    "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\" and comment.author != \"RemindMeBot\"): \n",
    "        if(count>=20):\n",
    "          break\n",
    "        else:\n",
    "          s = nltk.word_tokenize(comment.body)\n",
    "          mechanical_words.extend(s)\n",
    "          count+=1\n",
    "          print(\"Comment:\", comment.body)\n",
    "          print(s)\n",
    "\n",
    "          print(\"After Stemming: \")\n",
    "          # stem and lower each word and remove duplicates\n",
    "          mechanical_words_stem = [porter.stem(p.lower()) for p in s if p not in ignore_words]\n",
    "          mechanical_words_stem = sorted(list(set(mechanical_words_stem)))\n",
    "          print(\"length of a token in number of characters\")\n",
    "          for i in mechanical_words_stem:\n",
    "            print(len(i))\n",
    "            mechanical_token_length.append(len(i))\n",
    "          mechanical_sentence.extend(sent_tokenize(comment.body))\n",
    "          print(mechanical_words_stem)     \n",
    "          print(\"Number of sentences:\" , len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment \n",
    "          #mechanical_sentence_length.append(len(sent_tokenize(comment.body)))\n",
    "for i in mechanical_sentence:\n",
    "  mechanical_sentence_length.append(len(i.split()))\n",
    "print(mechanical_sentence)\n",
    "print(mechanical_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 7712,
     "status": "ok",
     "timestamp": 1602345750798,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "-kDTKLB5K8kM",
    "outputId": "567dd298-0d60-4891-b648-c3a4a152b14e"
   },
   "outputs": [],
   "source": [
    "print(\"r/programming\")\n",
    "program_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a']\n",
    "program_token_length = []\n",
    "program_sentence_length = []\n",
    "program_sentence = []\n",
    "count = 0\n",
    "for submission in reddit.subreddit('programming').new(limit=30):\n",
    "    for comment in submission.comments.list():\n",
    "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\"): \n",
    "        if(count>=20):\n",
    "          break\n",
    "        else:\n",
    "          p = nltk.word_tokenize(comment.body)\n",
    "          program_words.extend(p)\n",
    "          count+=1\n",
    "          print(\"Comment:\", comment.body)\n",
    "          print(p)\n",
    "\n",
    "          print(\"After Stemming: \")\n",
    "          # stem and lower each word and remove duplicates\n",
    "          program_words_stem = [porter.stem(x.lower()) for x in p if x not in ignore_words]\n",
    "          program_words_stem = sorted(list(set(program_words_stem)))\n",
    "          print(\"length of a token in number of characters\")\n",
    "          for i in program_words_stem:\n",
    "            print(len(i))\n",
    "            program_token_length.append(len(i))\n",
    "          program_sentence.extend(sent_tokenize(comment.body))\n",
    "          print(program_words_stem)\n",
    "          print(\"Number of sentences: \" , len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment \n",
    "          #program_sentence_length.append(len(sent_tokenize(comment.body)))  \n",
    "print(program_sentence) \n",
    "for i in program_sentence:\n",
    "  program_sentence_length.append(len(i.split()))\n",
    "print(program_sentence)\n",
    "print(program_sentence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "executionInfo": {
     "elapsed": 7555,
     "status": "ok",
     "timestamp": 1602422770486,
     "user": {
      "displayName": "mashly20",
      "photoUrl": "",
      "userId": "09924304379561144093"
     },
     "user_tz": -480
    },
    "id": "-LF1NOEaK8fS",
    "outputId": "9ca59054-2ace-479c-d3d0-cf18700d94e1"
   },
   "outputs": [],
   "source": [
    "print(\"r/mechanicalkeyboards\")\n",
    "mechanical_words = []\n",
    "ignore_words = ['?',',','.','\"','-','’','is','are','the','a']\n",
    "\n",
    "\n",
    "count = 0\n",
    "listofcomments = []\n",
    "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=30):\n",
    "  listofcomments+=submission.comments.list()\n",
    "while(True):\n",
    "  chosen_comment = random.choice(listofcomments)\n",
    "  if(chosen_comment.author != \"sneakpeek_bot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\" and chosen_comment.author != \"Reddit-Book-Bot\" and chosen_comment.author != \"RemindMeBot\"): \n",
    "      if(count <= 2):\n",
    "        count+=1\n",
    "        w = nltk.word_tokenize(chosen_comment.body)\n",
    "        print(nltk.pos_tag(w))\n",
    "      else:\n",
    "        break   \n",
    "  else:\n",
    "    continue\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "CZ4045.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
