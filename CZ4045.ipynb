{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CZ4045.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWzBiF5ooV3g",
        "outputId": "99027171-97d2-48d0-d548-6bcbdbf5d3bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from IPython import display\n",
        "import math\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "porter=PorterStemmer()\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style='darkgrid', context='talk', palette='Dark2')\n",
        "!pip install -q praw\n",
        "import praw\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import random\n",
        "reddit = praw.Reddit(client_id='H_TkAcrhaSZH4g',\n",
        "                     client_secret='qD3XYQe5fmWxDlHCT-8f36Iqd6c',\n",
        "                     user_agent='tlim045')\n",
        "headlines = set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kuDXEs7_q6AH",
        "outputId": "59f37d8c-4579-44fd-d3fa-76223cd7b1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"r/wallstreetbets\")\n",
        "wsb_words = []\n",
        "ignore_words = ['?',',','.','\"','-','â€™'] # TODO: remove profanities here\n",
        "wsb_chosen_comments = []\n",
        "count = 0\n",
        "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
        "  for comment in submission.comments.list():\n",
        "    if(comment.author != \"WSBVoteBot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\"): \n",
        "      if(count>=20):\n",
        "        break\n",
        "      else:\n",
        "        w = nltk.word_tokenize(comment.body)\n",
        "        wsb_words.extend(w)\n",
        "        count+=1\n",
        "        print(\"Comment:\", comment.body)\n",
        "        print(w)\n",
        "\n",
        "        print(\"After Stemming: \")\n",
        "        # stem and lower each word and remove duplicates\n",
        "        wsb_words_stem = [porter.stem(p.lower()) for p in w if p not in ignore_words]\n",
        "        wsb_words_stem = sorted(list(set(wsb_words_stem)))\n",
        "\n",
        "        print(wsb_words_stem)  \n",
        "        print(sent_tokenize(comment.body))\n",
        "        print(\"Number of sentences: \", len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment\n",
        "count = 0\n",
        "for submission in reddit.subreddit('wallstreetbets').new(limit=10):\n",
        "  listofcomments = submission.comments.list()\n",
        "  chosen_comment = random.choice(listofcomments)\n",
        "  if(chosen_comment.author != \"WSBVoteBot\" and chosen_comment.author != \"AutoModerator\" and chosen_comment.author != \"pickbot\"): \n",
        "      chosen_comment = random.choice(listofcomments)\n",
        "      if(count <= 2):\n",
        "\n",
        "        count+=1\n",
        "        w = nltk.word_tokenize(chosen_comment.body)\n",
        "        print(nltk.pos_tag(w))\n",
        "      else:\n",
        "        break\n",
        "  \n",
        "\n",
        "#for i in range(3):\n",
        "#  chosen = random.choice(comments_list)\n",
        "#  if chosen not in wsb_chosen_comments:\n",
        "#    wsb_chosen_comments.append(chosen)\n",
        "#    i += 1\n",
        "#for comments in wsb_chosen_comment:\n",
        "#  ntlk.pos_tag(comments)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r/wallstreetbets\n",
            "Comment: Good shit retard welcome home\n",
            "['Good', 'shit', 'retard', 'welcome', 'home']\n",
            "After Stemming: \n",
            "['good', 'home', 'retard', 'shit', 'welcom']\n",
            "['Good shit retard welcome home']\n",
            "Number of sentences:  1\n",
            "Comment: â€œThat is why embittered people find heroes and madmen a perennial source of fascination, for they have no fear of life or death. Both heroes and madmen are indifferent to danger and will forge ahead regardless of what other people say.â€\n",
            "Paulo Coelho\n",
            "['â€œ', 'That', 'is', 'why', 'embittered', 'people', 'find', 'heroes', 'and', 'madmen', 'a', 'perennial', 'source', 'of', 'fascination', ',', 'for', 'they', 'have', 'no', 'fear', 'of', 'life', 'or', 'death', '.', 'Both', 'heroes', 'and', 'madmen', 'are', 'indifferent', 'to', 'danger', 'and', 'will', 'forge', 'ahead', 'regardless', 'of', 'what', 'other', 'people', 'say.', 'â€', 'Paulo', 'Coelho']\n",
            "After Stemming: \n",
            "['a', 'ahead', 'and', 'are', 'both', 'coelho', 'danger', 'death', 'embitt', 'fascin', 'fear', 'find', 'for', 'forg', 'have', 'hero', 'indiffer', 'is', 'life', 'madmen', 'no', 'of', 'or', 'other', 'paulo', 'peopl', 'perenni', 'regardless', 'say.', 'sourc', 'that', 'they', 'to', 'what', 'whi', 'will', 'â€œ', 'â€']\n",
            "['â€œThat is why embittered people find heroes and madmen a perennial source of fascination, for they have no fear of life or death.', 'Both heroes and madmen are indifferent to danger and will forge ahead regardless of what other people say.â€\\nPaulo Coelho']\n",
            "Number of sentences:  2\n",
            "Comment: I fucking hate you. Congrats.\n",
            "I lost a fortune. Watch it moon next week\n",
            "['I', 'fucking', 'hate', 'you', '.', 'Congrats', '.', 'I', 'lost', 'a', 'fortune', '.', 'Watch', 'it', 'moon', 'next', 'week']\n",
            "After Stemming: \n",
            "['a', 'congrat', 'fortun', 'fuck', 'hate', 'i', 'it', 'lost', 'moon', 'next', 'watch', 'week', 'you']\n",
            "['I fucking hate you.', 'Congrats.', 'I lost a fortune.', 'Watch it moon next week']\n",
            "Number of sentences:  4\n",
            "Comment: Re-roll if you got what it takes to be rich\n",
            "['Re-roll', 'if', 'you', 'got', 'what', 'it', 'takes', 'to', 'be', 'rich']\n",
            "After Stemming: \n",
            "['be', 'got', 'if', 'it', 're-rol', 'rich', 'take', 'to', 'what', 'you']\n",
            "['Re-roll if you got what it takes to be rich']\n",
            "Number of sentences:  1\n",
            "Comment: FINACETALITY\n",
            "You lose\n",
            "\n",
            "Oh wait... its not monday yet. Then GL ofc ðŸ˜ðŸ‘ ... and in case that you realised your gains: CG & FU\n",
            "['FINACETALITY', 'You', 'lose', 'Oh', 'wait', '...', 'its', 'not', 'monday', 'yet', '.', 'Then', 'GL', 'ofc', 'ðŸ˜ðŸ‘', '...', 'and', 'in', 'case', 'that', 'you', 'realised', 'your', 'gains', ':', 'CG', '&', 'FU']\n",
            "After Stemming: \n",
            "['&', '...', ':', 'and', 'case', 'cg', 'finacet', 'fu', 'gain', 'gl', 'in', 'it', 'lose', 'monday', 'not', 'ofc', 'oh', 'realis', 'that', 'then', 'wait', 'yet', 'you', 'your', 'ðŸ˜ðŸ‘']\n",
            "['FINACETALITY\\nYou lose\\n\\nOh wait... its not monday yet.', 'Then GL ofc ðŸ˜ðŸ‘ ... and in case that you realised your gains: CG & FU']\n",
            "Number of sentences:  2\n",
            "Comment: You pocketed $7.35k retard. But congrats!\n",
            "['You', 'pocketed', '$', '7.35k', 'retard', '.', 'But', 'congrats', '!']\n",
            "After Stemming: \n",
            "['!', '$', '7.35k', 'but', 'congrat', 'pocket', 'retard', 'you']\n",
            "['You pocketed $7.35k retard.', 'But congrats!']\n",
            "Number of sentences:  2\n",
            "Comment: tHeTa GaNg ðŸ¥´ðŸ¥´\n",
            "['tHeTa', 'GaNg', '\\U0001f974\\U0001f974']\n",
            "After Stemming: \n",
            "['gang', 'theta', '\\U0001f974\\U0001f974']\n",
            "['tHeTa GaNg \\U0001f974\\U0001f974']\n",
            "Number of sentences:  1\n",
            "Comment: you son of a bitch, iâ€™m out\n",
            "['you', 'son', 'of', 'a', 'bitch', ',', 'i', 'â€™', 'm', 'out']\n",
            "After Stemming: \n",
            "['a', 'bitch', 'i', 'm', 'of', 'out', 'son', 'you']\n",
            "['you son of a bitch, iâ€™m out']\n",
            "Number of sentences:  1\n",
            "Comment: This isnâ€™t a bad move at all, $wmt is a tank of a ticker imo\n",
            "['This', 'isn', 'â€™', 't', 'a', 'bad', 'move', 'at', 'all', ',', '$', 'wmt', 'is', 'a', 'tank', 'of', 'a', 'ticker', 'imo']\n",
            "After Stemming: \n",
            "['$', 'a', 'all', 'at', 'bad', 'imo', 'is', 'isn', 'move', 'of', 't', 'tank', 'thi', 'ticker', 'wmt']\n",
            "['This isnâ€™t a bad move at all, $wmt is a tank of a ticker imo']\n",
            "Number of sentences:  1\n",
            "Comment: My wife left me\n",
            "['My', 'wife', 'left', 'me']\n",
            "After Stemming: \n",
            "['left', 'me', 'my', 'wife']\n",
            "['My wife left me']\n",
            "Number of sentences:  1\n",
            "Comment: I mean yeah thatâ€™s cool and all but isnâ€™t Target killing Wally-world on every major comparison\n",
            "['I', 'mean', 'yeah', 'that', 'â€™', 's', 'cool', 'and', 'all', 'but', 'isn', 'â€™', 't', 'Target', 'killing', 'Wally-world', 'on', 'every', 'major', 'comparison']\n",
            "After Stemming: \n",
            "['all', 'and', 'but', 'comparison', 'cool', 'everi', 'i', 'isn', 'kill', 'major', 'mean', 'on', 's', 't', 'target', 'that', 'wally-world', 'yeah']\n",
            "['I mean yeah thatâ€™s cool and all but isnâ€™t Target killing Wally-world on every major comparison']\n",
            "Number of sentences:  1\n",
            "Comment: Remember there's no options okay for this but Walmart is trying hard to become a health giant. They are slowing building clinics in the SE.\n",
            "['Remember', 'there', \"'s\", 'no', 'options', 'okay', 'for', 'this', 'but', 'Walmart', 'is', 'trying', 'hard', 'to', 'become', 'a', 'health', 'giant', '.', 'They', 'are', 'slowing', 'building', 'clinics', 'in', 'the', 'SE', '.']\n",
            "After Stemming: \n",
            "[\"'s\", 'a', 'are', 'becom', 'build', 'but', 'clinic', 'for', 'giant', 'hard', 'health', 'in', 'is', 'no', 'okay', 'option', 'rememb', 'se', 'slow', 'the', 'there', 'they', 'thi', 'to', 'tri', 'walmart']\n",
            "[\"Remember there's no options okay for this but Walmart is trying hard to become a health giant.\", 'They are slowing building clinics in the SE.']\n",
            "Number of sentences:  2\n",
            "Comment: WMT 135c SEP 2022ðŸ˜Ž\n",
            "['WMT', '135c', 'SEP', '2022ðŸ˜Ž']\n",
            "After Stemming: \n",
            "['135c', '2022ðŸ˜Ž', 'sep', 'wmt']\n",
            "['WMT 135c SEP 2022ðŸ˜Ž']\n",
            "Number of sentences:  1\n",
            "Comment: Is this a fucking joke ?\n",
            "['Is', 'this', 'a', 'fucking', 'joke', '?']\n",
            "After Stemming: \n",
            "['a', 'fuck', 'is', 'joke', 'thi']\n",
            "['Is this a fucking joke ?']\n",
            "Number of sentences:  1\n",
            "Comment: Just buy a new wife\n",
            "['Just', 'buy', 'a', 'new', 'wife']\n",
            "After Stemming: \n",
            "['a', 'buy', 'just', 'new', 'wife']\n",
            "['Just buy a new wife']\n",
            "Number of sentences:  1\n",
            "Comment: and wmt left south dakota, so...\n",
            "['and', 'wmt', 'left', 'south', 'dakota', ',', 'so', '...']\n",
            "After Stemming: \n",
            "['...', 'and', 'dakota', 'left', 'so', 'south', 'wmt']\n",
            "['and wmt left south dakota, so...']\n",
            "Number of sentences:  1\n",
            "Comment: WMT has options.\n",
            "\n",
            "Yes, WMT is opening up small clinics thought the U.S.  They want to grab a share of the $3.6Â trillion in health spending by leveraging its 150 million weekly shoppers.\n",
            "\n",
            "Health check ups for $30 and teeth cleaning for $25\n",
            "['WMT', 'has', 'options', '.', 'Yes', ',', 'WMT', 'is', 'opening', 'up', 'small', 'clinics', 'thought', 'the', 'U.S', '.', 'They', 'want', 'to', 'grab', 'a', 'share', 'of', 'the', '$', '3.6', 'trillion', 'in', 'health', 'spending', 'by', 'leveraging', 'its', '150', 'million', 'weekly', 'shoppers', '.', 'Health', 'check', 'ups', 'for', '$', '30', 'and', 'teeth', 'cleaning', 'for', '$', '25']\n",
            "After Stemming: \n",
            "['$', '150', '25', '3.6', '30', 'a', 'and', 'by', 'check', 'clean', 'clinic', 'for', 'grab', 'ha', 'health', 'in', 'is', 'it', 'leverag', 'million', 'of', 'open', 'option', 'share', 'shopper', 'small', 'spend', 'teeth', 'the', 'they', 'thought', 'to', 'trillion', 'u.', 'up', 'want', 'weekli', 'wmt', 'ye']\n",
            "['WMT has options.', 'Yes, WMT is opening up small clinics thought the U.S.', 'They want to grab a share of the $3.6\\xa0trillion in health spending by leveraging its 150 million weekly shoppers.', 'Health check ups for $30 and teeth cleaning for $25']\n",
            "Number of sentences:  4\n",
            "Comment: The strikes might be a bit high in the short term but don't underestimate walmart. They have the firepower and balls to take on amazon.\n",
            "['The', 'strikes', 'might', 'be', 'a', 'bit', 'high', 'in', 'the', 'short', 'term', 'but', 'do', \"n't\", 'underestimate', 'walmart', '.', 'They', 'have', 'the', 'firepower', 'and', 'balls', 'to', 'take', 'on', 'amazon', '.']\n",
            "After Stemming: \n",
            "['a', 'amazon', 'and', 'ball', 'be', 'bit', 'but', 'do', 'firepow', 'have', 'high', 'in', 'might', \"n't\", 'on', 'short', 'strike', 'take', 'term', 'the', 'they', 'to', 'underestim', 'walmart']\n",
            "[\"The strikes might be a bit high in the short term but don't underestimate walmart.\", 'They have the firepower and balls to take on amazon.']\n",
            "Number of sentences:  2\n",
            "Comment: Calls on $rope\n",
            "['Calls', 'on', '$', 'rope']\n",
            "After Stemming: \n",
            "['$', 'call', 'on', 'rope']\n",
            "['Calls on $rope']\n",
            "Number of sentences:  1\n",
            "Comment: Right they have options but I meant a long long term play. Thinking like 2025 and beyond. They will wreck the health system and take out cvs/walgreens clinics if they can get their ducks in a row. Imagine if they partnered with apple and got people intergrated into a health ecosystem. Wmt would have a $4T mkt cap.\n",
            "\n",
            "\n",
            "I'm bullish on them long term because they always find a way to win.\n",
            "\n",
            "\n",
            "But their best play is an hour of therapy for $40 or something. Lol. Get some groceries and talk to a shrink for $40. It's going to be a gold mine\n",
            "['Right', 'they', 'have', 'options', 'but', 'I', 'meant', 'a', 'long', 'long', 'term', 'play', '.', 'Thinking', 'like', '2025', 'and', 'beyond', '.', 'They', 'will', 'wreck', 'the', 'health', 'system', 'and', 'take', 'out', 'cvs/walgreens', 'clinics', 'if', 'they', 'can', 'get', 'their', 'ducks', 'in', 'a', 'row', '.', 'Imagine', 'if', 'they', 'partnered', 'with', 'apple', 'and', 'got', 'people', 'intergrated', 'into', 'a', 'health', 'ecosystem', '.', 'Wmt', 'would', 'have', 'a', '$', '4T', 'mkt', 'cap', '.', 'I', \"'m\", 'bullish', 'on', 'them', 'long', 'term', 'because', 'they', 'always', 'find', 'a', 'way', 'to', 'win', '.', 'But', 'their', 'best', 'play', 'is', 'an', 'hour', 'of', 'therapy', 'for', '$', '40', 'or', 'something', '.', 'Lol', '.', 'Get', 'some', 'groceries', 'and', 'talk', 'to', 'a', 'shrink', 'for', '$', '40', '.', 'It', \"'s\", 'going', 'to', 'be', 'a', 'gold', 'mine']\n",
            "After Stemming: \n",
            "['$', \"'m\", \"'s\", '2025', '40', '4t', 'a', 'alway', 'an', 'and', 'appl', 'be', 'becaus', 'best', 'beyond', 'bullish', 'but', 'can', 'cap', 'clinic', 'cvs/walgreen', 'duck', 'ecosystem', 'find', 'for', 'get', 'go', 'gold', 'got', 'groceri', 'have', 'health', 'hour', 'i', 'if', 'imagin', 'in', 'intergr', 'into', 'is', 'it', 'like', 'lol', 'long', 'meant', 'mine', 'mkt', 'of', 'on', 'option', 'or', 'out', 'partner', 'peopl', 'play', 'right', 'row', 'shrink', 'some', 'someth', 'system', 'take', 'talk', 'term', 'the', 'their', 'them', 'therapi', 'they', 'think', 'to', 'way', 'will', 'win', 'with', 'wmt', 'would', 'wreck']\n",
            "['Right they have options but I meant a long long term play.', 'Thinking like 2025 and beyond.', 'They will wreck the health system and take out cvs/walgreens clinics if they can get their ducks in a row.', 'Imagine if they partnered with apple and got people intergrated into a health ecosystem.', 'Wmt would have a $4T mkt cap.', \"I'm bullish on them long term because they always find a way to win.\", 'But their best play is an hour of therapy for $40 or something.', 'Lol.', 'Get some groceries and talk to a shrink for $40.', \"It's going to be a gold mine\"]\n",
            "Number of sentences:  10\n",
            "[('I', 'PRP'), ('fucking', 'VBG'), ('hate', 'NN'), ('you', 'PRP'), ('.', '.'), ('Congrats', 'NNPS'), ('.', '.'), ('I', 'PRP'), ('lost', 'VBD'), ('a', 'DT'), ('fortune', 'NN'), ('.', '.'), ('Watch', 'VB'), ('it', 'PRP'), ('moon', 'RB'), ('next', 'JJ'), ('week', 'NN')]\n",
            "[('Remember', 'NNP'), ('there', 'EX'), (\"'s\", 'VBZ'), ('no', 'DT'), ('options', 'NNS'), ('okay', 'VBP'), ('for', 'IN'), ('this', 'DT'), ('but', 'CC'), ('Walmart', 'NNP'), ('is', 'VBZ'), ('trying', 'VBG'), ('hard', 'JJ'), ('to', 'TO'), ('become', 'VB'), ('a', 'DT'), ('health', 'NN'), ('giant', 'NN'), ('.', '.'), ('They', 'PRP'), ('are', 'VBP'), ('slowing', 'VBG'), ('building', 'VBG'), ('clinics', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('SE', 'NNP'), ('.', '.')]\n",
            "[('Link', 'VB'), ('the', 'DT'), ('video', 'NN'), ('or', 'CC'), ('fuck', 'NN'), ('you', 'PRP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D91DMF5q5o-",
        "outputId": "5d5ca7fd-9f26-4ae4-963a-267fa3f10c79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"r/mechanicalkeyboards\")\n",
        "singapore_words = []\n",
        "ignore_words = ['?',',','.','\"','-','â€™'] \n",
        "count = 0\n",
        "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=10):\n",
        "    for comment in submission.comments.list():\n",
        "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\"): \n",
        "        if(count>=20):\n",
        "          break\n",
        "        else:\n",
        "          s = nltk.word_tokenize(comment.body)\n",
        "          singapore_words.extend(s)\n",
        "          count+=1\n",
        "          print(\"Comment:\", comment.body)\n",
        "          print(s)\n",
        "\n",
        "          print(\"After Stemming: \")\n",
        "          # stem and lower each word and remove duplicates\n",
        "          singapore_words_stem = [porter.stem(p.lower()) for p in s if p not in ignore_words]\n",
        "          singapore_words_stem = sorted(list(set(singapore_words_stem)))\n",
        "\n",
        "          print(singapore_words_stem)     \n",
        "          print(len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment \n",
        "\n",
        "count = 0\n",
        "\n",
        "for submission in reddit.subreddit('mechanicalkeyboards').new(limit=10):\n",
        "  listofcomments = submission.comments.list()\n",
        "  if(len(listofcomments) == 0):\n",
        "    continue\n",
        "  chosen_comment = random.choice(listofcomments)\n",
        "  if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\" and comment.author != \"pickbot\"): \n",
        "      chosen_comment = random.choice(listofcomments)\n",
        "      if(count <= 2):\n",
        "\n",
        "        count+=1\n",
        "        w = nltk.word_tokenize(chosen_comment.body)\n",
        "        print(nltk.pos_tag(w))\n",
        "      else:\n",
        "        break   \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r/mechanicalkeyboards\n",
            "Comment: Had this happen with my Rama. I had to reflash my pcb.\n",
            "['Had', 'this', 'happen', 'with', 'my', 'Rama', '.', 'I', 'had', 'to', 'reflash', 'my', 'pcb', '.']\n",
            "After Stemming: \n",
            "['had', 'happen', 'i', 'my', 'pcb', 'rama', 'reflash', 'thi', 'to', 'with']\n",
            "2\n",
            "Comment: no clue its not mechanical . maybe try surface pro subreddit\n",
            "['no', 'clue', 'its', 'not', 'mechanical', '.', 'maybe', 'try', 'surface', 'pro', 'subreddit']\n",
            "After Stemming: \n",
            "['clue', 'it', 'mayb', 'mechan', 'no', 'not', 'pro', 'subreddit', 'surfac', 'tri']\n",
            "2\n",
            "Comment: I mean go get some nickels... Each one is 5 grams.\n",
            "['I', 'mean', 'go', 'get', 'some', 'nickels', '...', 'Each', 'one', 'is', '5', 'grams', '.']\n",
            "After Stemming: \n",
            "['...', '5', 'each', 'get', 'go', 'gram', 'i', 'is', 'mean', 'nickel', 'one', 'some']\n",
            "1\n",
            "Comment: Do you want all thin bezel or all thick bezel. Thin is easy, thick will be expensive\n",
            "['Do', 'you', 'want', 'all', 'thin', 'bezel', 'or', 'all', 'thick', 'bezel', '.', 'Thin', 'is', 'easy', ',', 'thick', 'will', 'be', 'expensive']\n",
            "After Stemming: \n",
            "['all', 'be', 'bezel', 'do', 'easi', 'expens', 'is', 'or', 'thick', 'thin', 'want', 'will', 'you']\n",
            "2\n",
            "Comment: Theres the NK65, but thats a 65%, it adds thearrow keys and a few others, there is also the XD60 and XD65 as well\n",
            "['Theres', 'the', 'NK65', ',', 'but', 'thats', 'a', '65', '%', ',', 'it', 'adds', 'thearrow', 'keys', 'and', 'a', 'few', 'others', ',', 'there', 'is', 'also', 'the', 'XD60', 'and', 'XD65', 'as', 'well']\n",
            "After Stemming: \n",
            "['%', '65', 'a', 'add', 'also', 'and', 'as', 'but', 'few', 'is', 'it', 'key', 'nk65', 'other', 'that', 'the', 'thearrow', 'there', 'well', 'xd60', 'xd65']\n",
            "1\n",
            "Comment: Thin/Medium white\n",
            "['Thin/Medium', 'white']\n",
            "After Stemming: \n",
            "['thin/medium', 'white']\n",
            "1\n",
            "Comment: Shout out to u/kevinzavier for this bluerite boi!\n",
            "['Shout', 'out', 'to', 'u/kevinzavier', 'for', 'this', 'bluerite', 'boi', '!']\n",
            "After Stemming: \n",
            "['!', 'bluerit', 'boi', 'for', 'out', 'shout', 'thi', 'to', 'u/kevinzavi']\n",
            "1\n",
            "Comment: https://www.reddit.com/r/CustomKeyboards/comments/hkpy7s/the_no_plastic_rule_should_also_apply_to_caps/ somone did annodized alum cost 4 bucks a cap but i think he did most of the work himself so be ready for that kind of cost\n",
            "['https', ':', '//www.reddit.com/r/CustomKeyboards/comments/hkpy7s/the_no_plastic_rule_should_also_apply_to_caps/', 'somone', 'did', 'annodized', 'alum', 'cost', '4', 'bucks', 'a', 'cap', 'but', 'i', 'think', 'he', 'did', 'most', 'of', 'the', 'work', 'himself', 'so', 'be', 'ready', 'for', 'that', 'kind', 'of', 'cost']\n",
            "After Stemming: \n",
            "['//www.reddit.com/r/customkeyboards/comments/hkpy7s/the_no_plastic_rule_should_also_apply_to_caps/', '4', ':', 'a', 'alum', 'annod', 'be', 'buck', 'but', 'cap', 'cost', 'did', 'for', 'he', 'himself', 'http', 'i', 'kind', 'most', 'of', 'readi', 'so', 'somon', 'that', 'the', 'think', 'work']\n",
            "1\n",
            "Comment: [Looks like they're in stock on their website?](https://www.nizkeyboard.com/products/2019-new-micro-82-ec-keyboard-s-ble-ble-rgb-or-non-rgb)\n",
            "['[', 'Looks', 'like', 'they', \"'re\", 'in', 'stock', 'on', 'their', 'website', '?', ']', '(', 'https', ':', '//www.nizkeyboard.com/products/2019-new-micro-82-ec-keyboard-s-ble-ble-rgb-or-non-rgb', ')']\n",
            "After Stemming: \n",
            "[\"'re\", '(', ')', '//www.nizkeyboard.com/products/2019-new-micro-82-ec-keyboard-s-ble-ble-rgb-or-non-rgb', ':', '[', ']', 'http', 'in', 'like', 'look', 'on', 'stock', 'their', 'they', 'websit']\n",
            "2\n",
            "Comment: You can build a custom keyboard without a PCB, using a Teensy module and switches in a plate, hand-wired together. There's an online service that lets you design a board, and you can save the data and send it to a service that will cut a plate for you.\n",
            "['You', 'can', 'build', 'a', 'custom', 'keyboard', 'without', 'a', 'PCB', ',', 'using', 'a', 'Teensy', 'module', 'and', 'switches', 'in', 'a', 'plate', ',', 'hand-wired', 'together', '.', 'There', \"'s\", 'an', 'online', 'service', 'that', 'lets', 'you', 'design', 'a', 'board', ',', 'and', 'you', 'can', 'save', 'the', 'data', 'and', 'send', 'it', 'to', 'a', 'service', 'that', 'will', 'cut', 'a', 'plate', 'for', 'you', '.']\n",
            "After Stemming: \n",
            "[\"'s\", 'a', 'an', 'and', 'board', 'build', 'can', 'custom', 'cut', 'data', 'design', 'for', 'hand-wir', 'in', 'it', 'keyboard', 'let', 'modul', 'onlin', 'pcb', 'plate', 'save', 'send', 'servic', 'switch', 'teensi', 'that', 'the', 'there', 'to', 'togeth', 'use', 'will', 'without', 'you']\n",
            "2\n",
            "Comment: You can visit [Keyboard University](https://keyboard.university/) to learn about the basics.\n",
            "\n",
            "You would need to acquire the following items:\n",
            "\n",
            "* Case\n",
            "* PCB\n",
            "* Plate\n",
            "* Stabilizers\n",
            "* Cable\n",
            "* Switches\n",
            "* Keycaps\n",
            "\n",
            "Some vendors sell \"kits\" which may contain several of the items listed above, which can help new people like you.\n",
            "\n",
            "If you want to build it completely from scratch, then you'll need knowledge/skill of wood/metal working for the case/plate and electronics for the PCB. Even then, I'd rather emulate existing ones rather than start out fresh.\n",
            "['You', 'can', 'visit', '[', 'Keyboard', 'University', ']', '(', 'https', ':', '//keyboard.university/', ')', 'to', 'learn', 'about', 'the', 'basics', '.', 'You', 'would', 'need', 'to', 'acquire', 'the', 'following', 'items', ':', '*', 'Case', '*', 'PCB', '*', 'Plate', '*', 'Stabilizers', '*', 'Cable', '*', 'Switches', '*', 'Keycaps', 'Some', 'vendors', 'sell', '``', 'kits', \"''\", 'which', 'may', 'contain', 'several', 'of', 'the', 'items', 'listed', 'above', ',', 'which', 'can', 'help', 'new', 'people', 'like', 'you', '.', 'If', 'you', 'want', 'to', 'build', 'it', 'completely', 'from', 'scratch', ',', 'then', 'you', \"'ll\", 'need', 'knowledge/skill', 'of', 'wood/metal', 'working', 'for', 'the', 'case/plate', 'and', 'electronics', 'for', 'the', 'PCB', '.', 'Even', 'then', ',', 'I', \"'d\", 'rather', 'emulate', 'existing', 'ones', 'rather', 'than', 'start', 'out', 'fresh', '.']\n",
            "After Stemming: \n",
            "[\"''\", \"'d\", \"'ll\", '(', ')', '*', '//keyboard.university/', ':', '[', ']', '``', 'about', 'abov', 'acquir', 'and', 'basic', 'build', 'cabl', 'can', 'case', 'case/pl', 'complet', 'contain', 'electron', 'emul', 'even', 'exist', 'follow', 'for', 'fresh', 'from', 'help', 'http', 'i', 'if', 'it', 'item', 'keyboard', 'keycap', 'kit', 'knowledge/skil', 'learn', 'like', 'list', 'may', 'need', 'new', 'of', 'one', 'out', 'pcb', 'peopl', 'plate', 'rather', 'scratch', 'sell', 'sever', 'some', 'stabil', 'start', 'switch', 'than', 'the', 'then', 'to', 'univers', 'vendor', 'visit', 'want', 'which', 'wood/met', 'work', 'would', 'you']\n",
            "4\n",
            "Comment: I think you buy all of the parts separately and combine them and solder them.\n",
            "['I', 'think', 'you', 'buy', 'all', 'of', 'the', 'parts', 'separately', 'and', 'combine', 'them', 'and', 'solder', 'them', '.']\n",
            "After Stemming: \n",
            "['all', 'and', 'buy', 'combin', 'i', 'of', 'part', 'separ', 'solder', 'the', 'them', 'think', 'you']\n",
            "1\n",
            "Comment: Are you talking about buying parts and putting them together, or designing everything from the ground up?\n",
            "['Are', 'you', 'talking', 'about', 'buying', 'parts', 'and', 'putting', 'them', 'together', ',', 'or', 'designing', 'everything', 'from', 'the', 'ground', 'up', '?']\n",
            "After Stemming: \n",
            "['about', 'and', 'are', 'buy', 'design', 'everyth', 'from', 'ground', 'or', 'part', 'put', 'talk', 'the', 'them', 'togeth', 'up', 'you']\n",
            "1\n",
            "Comment: I think he's talking about truly custom. Building or designing parts himself\n",
            "['I', 'think', 'he', \"'s\", 'talking', 'about', 'truly', 'custom', '.', 'Building', 'or', 'designing', 'parts', 'himself']\n",
            "After Stemming: \n",
            "[\"'s\", 'about', 'build', 'custom', 'design', 'he', 'himself', 'i', 'or', 'part', 'talk', 'think', 'truli']\n",
            "2\n",
            "[('Hi', 'NNP'), (',', ','), ('it', 'PRP'), ('appears', 'VBZ'), ('you', 'PRP'), ('may', 'MD'), ('be', 'VB'), ('new', 'JJ'), ('to', 'TO'), ('this', 'DT'), ('subreddit', 'NN'), ('!', '.'), ('Please', 'NNP'), ('check', 'VB'), ('out', 'RP'), ('the', 'DT'), ('[', 'NN'), ('wiki', 'NN'), (']', 'NNP'), ('(', '('), ('https', 'NN'), (':', ':'), ('//www.reddit.com/r/MechanicalKeyboards/wiki/index', 'NN'), (')', ')'), ('for', 'IN'), ('general', 'JJ'), ('information', 'NN'), ('about', 'IN'), ('mechanical', 'JJ'), ('keyboards', 'NNS'), ('and', 'CC'), ('consider', 'VB'), ('posting', 'VBG'), ('questions', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('[', 'JJ'), ('daily', 'JJ'), ('sticky', 'JJ'), ('post', 'NN'), ('at', 'IN'), ('the', 'DT'), ('top', 'NN'), ('of', 'IN'), ('the', 'DT'), ('subreddit', 'NN'), (']', 'NNP'), ('(', '('), ('https', 'NN'), (':', ':'), ('//www.reddit.com/r/mechanicalkeyboards/hot', 'NN'), (')', ')'), ('for', 'IN'), ('any', 'DT'), ('smaller', 'JJR'), ('questions', 'NNS'), ('.', '.'), ('*I', 'VB'), ('am', 'VBP'), ('a', 'DT'), ('bot', 'NN'), (',', ','), ('and', 'CC'), ('this', 'DT'), ('action', 'NN'), ('was', 'VBD'), ('performed', 'VBN'), ('automatically', 'RB'), ('.', '.'), ('Please', 'NNP'), ('[', 'NNP'), ('contact', 'VB'), ('the', 'DT'), ('moderators', 'NNS'), ('of', 'IN'), ('this', 'DT'), ('subreddit', 'NN'), (']', 'NNP'), ('(', '('), ('/message/compose/', 'NNP'), ('?', '.'), ('to=/r/MechanicalKeyboards', 'NNS'), (')', ')'), ('if', 'IN'), ('you', 'PRP'), ('have', 'VBP'), ('any', 'DT'), ('questions', 'NNS'), ('or', 'CC'), ('concerns', 'NNS'), ('.', '.'), ('*', 'NN')]\n",
            "[('Had', 'VBD'), ('this', 'DT'), ('happen', 'NN'), ('with', 'IN'), ('my', 'PRP$'), ('Rama', 'NNP'), ('.', '.'), ('I', 'PRP'), ('had', 'VBD'), ('to', 'TO'), ('reflash', 'VB'), ('my', 'PRP$'), ('pcb', 'NN'), ('.', '.')]\n",
            "[('I', 'PRP'), ('mean', 'VBP'), ('go', 'VB'), ('get', 'VB'), ('some', 'DT'), ('nickels', 'NNS'), ('...', ':'), ('Each', 'DT'), ('one', 'CD'), ('is', 'VBZ'), ('5', 'CD'), ('grams', 'NNS'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofZ5JNI2C2_1",
        "outputId": "58f82f58-62a0-4e43-a791-e0e76cbb7828",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"r/programming\")\n",
        "program_words = []\n",
        "ignore_words = ['?',',','.','\"','-','â€™']\n",
        "count = 0\n",
        "for submission in reddit.subreddit('programming').new(limit=30):\n",
        "    for comment in submission.comments.list():\n",
        "      if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\"): \n",
        "        if(count>=20):\n",
        "          break\n",
        "        else:\n",
        "          p = nltk.word_tokenize(comment.body)\n",
        "          program_words.extend(p)\n",
        "          count+=1\n",
        "          print(\"Comment:\", comment.body)\n",
        "          print(p)\n",
        "\n",
        "          print(\"After Stemming: \")\n",
        "          # stem and lower each word and remove duplicates\n",
        "          program_words_stem = [porter.stem(x.lower()) for x in p if x not in ignore_words]\n",
        "          program_words_stem = sorted(list(set(program_words_stem)))\n",
        "\n",
        "          print(len(sent_tokenize(comment.body))) #Prints the number of sentences in each comment    \n",
        "count = 0\n",
        "for submission in reddit.subreddit('programming').new(limit=10):\n",
        "  listofcomments = submission.comments.list()\n",
        "  if(len(listofcomments) == 0):\n",
        "    continue\n",
        "  chosen_comment = random.choice(listofcomments)\n",
        "  if(comment.author != \"sneakpeek_bot\" and comment.author != \"AutoModerator\"): \n",
        "      chosen_comment = random.choice(listofcomments)\n",
        "      if(count <= 2):\n",
        "\n",
        "        count+=1\n",
        "        w = nltk.word_tokenize(chosen_comment.body)\n",
        "        print(nltk.pos_tag(w))\n",
        "      else:\n",
        "        break        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r/programming\n",
            "Comment: Spam in programming\n",
            "['Spam', 'in', 'programming']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: And if you don't feel you can contribute code or participate at that level yet - edit documentation. Most all open source projects need documentation help.\n",
            "['And', 'if', 'you', 'do', \"n't\", 'feel', 'you', 'can', 'contribute', 'code', 'or', 'participate', 'at', 'that', 'level', 'yet', '-', 'edit', 'documentation', '.', 'Most', 'all', 'open', 'source', 'projects', 'need', 'documentation', 'help', '.']\n",
            "After Stemming: \n",
            "2\n",
            "Comment: Most of all these hats are just \"presh 'cos\"\n",
            "['Most', 'of', 'all', 'these', 'hats', 'are', 'just', '``', 'presh', \"'cos\", \"''\"]\n",
            "After Stemming: \n",
            "1\n",
            "Comment: r/testflight\n",
            "['r/testflight']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: This is a brilliant idea. With enough commodity sensors placed at a reasonable distance with overlapping fields of view you would be able to pretty accurately estimate distance and speed of objects. Add in a couple of high resolution telephoto lenses on automated mounts and you could even capture images of the objects.\n",
            "['This', 'is', 'a', 'brilliant', 'idea', '.', 'With', 'enough', 'commodity', 'sensors', 'placed', 'at', 'a', 'reasonable', 'distance', 'with', 'overlapping', 'fields', 'of', 'view', 'you', 'would', 'be', 'able', 'to', 'pretty', 'accurately', 'estimate', 'distance', 'and', 'speed', 'of', 'objects', '.', 'Add', 'in', 'a', 'couple', 'of', 'high', 'resolution', 'telephoto', 'lenses', 'on', 'automated', 'mounts', 'and', 'you', 'could', 'even', 'capture', 'images', 'of', 'the', 'objects', '.']\n",
            "After Stemming: \n",
            "3\n",
            "Comment: There are some software developers writing code\n",
            "['There', 'are', 'some', 'software', 'developers', 'writing', 'code']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: So they built a UFO hunting device and the camera is b/w and 320x240 pixels and 1 frame per second ?\n",
            "['So', 'they', 'built', 'a', 'UFO', 'hunting', 'device', 'and', 'the', 'camera', 'is', 'b/w', 'and', '320x240', 'pixels', 'and', '1', 'frame', 'per', 'second', '?']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: \"Sky Hub\"... nice try there, SkyNet.\n",
            "['``', 'Sky', 'Hub', \"''\", '...', 'nice', 'try', 'there', ',', 'SkyNet', '.']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: Sounds like pornhub but for ET\n",
            "['Sounds', 'like', 'pornhub', 'but', 'for', 'ET']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: I wonder if the data is going to be analysed in a true scientific way or just be another case of \"we don't know what it is in this white and black video so must be aliens\"\n",
            "['I', 'wonder', 'if', 'the', 'data', 'is', 'going', 'to', 'be', 'analysed', 'in', 'a', 'true', 'scientific', 'way', 'or', 'just', 'be', 'another', 'case', 'of', '``', 'we', 'do', \"n't\", 'know', 'what', 'it', 'is', 'in', 'this', 'white', 'and', 'black', 'video', 'so', 'must', 'be', 'aliens', \"''\"]\n",
            "After Stemming: \n",
            "1\n",
            "Comment: r/ufos might enjoy this\n",
            "['r/ufos', 'might', 'enjoy', 'this']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: Fun\n",
            "['Fun']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: I wonder if theyâ€™ll run into trademark issues with BSkyB. Their broadband routers are rather famously known as â€œSky Hubâ€s.\n",
            "['I', 'wonder', 'if', 'they', 'â€™', 'll', 'run', 'into', 'trademark', 'issues', 'with', 'BSkyB', '.', 'Their', 'broadband', 'routers', 'are', 'rather', 'famously', 'known', 'as', 'â€œ', 'Sky', 'Hub', 'â€', 's', '.']\n",
            "After Stemming: \n",
            "2\n",
            "Comment: Iâ€™m really not sure what you can tell from the images of a few dots in the sky. Depending on the sensor and itâ€™s quality,a lot of these could just be visual artifacts. Seems like an interesting project though.\n",
            "['I', 'â€™', 'm', 'really', 'not', 'sure', 'what', 'you', 'can', 'tell', 'from', 'the', 'images', 'of', 'a', 'few', 'dots', 'in', 'the', 'sky', '.', 'Depending', 'on', 'the', 'sensor', 'and', 'it', 'â€™', 's', 'quality', ',', 'a', 'lot', 'of', 'these', 'could', 'just', 'be', 'visual', 'artifacts', '.', 'Seems', 'like', 'an', 'interesting', 'project', 'though', '.']\n",
            "After Stemming: \n",
            "3\n",
            "Comment: If you really cared about this, wouldn't renting observation time on satellites be better?\n",
            "['If', 'you', 'really', 'cared', 'about', 'this', ',', 'would', \"n't\", 'renting', 'observation', 'time', 'on', 'satellites', 'be', 'better', '?']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: What a waste of resources and time.\n",
            "['What', 'a', 'waste', 'of', 'resources', 'and', 'time', '.']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: Wow. What nutjobs. This is a dumb project\n",
            "['Wow', '.', 'What', 'nutjobs', '.', 'This', 'is', 'a', 'dumb', 'project']\n",
            "After Stemming: \n",
            "3\n",
            "Comment: One day a group like this will catch an alien with what would be the equivalent of catching a human with a fishing net and itâ€™s gonna freak em out!\n",
            "['One', 'day', 'a', 'group', 'like', 'this', 'will', 'catch', 'an', 'alien', 'with', 'what', 'would', 'be', 'the', 'equivalent', 'of', 'catching', 'a', 'human', 'with', 'a', 'fishing', 'net', 'and', 'it', 'â€™', 's', 'gon', 'na', 'freak', 'em', 'out', '!']\n",
            "After Stemming: \n",
            "1\n",
            "Comment: Their website is https://skyhub.org. There's a chat, a wiki, a link to the source code and some other info on there.\n",
            "['Their', 'website', 'is', 'https', ':', '//skyhub.org', '.', 'There', \"'s\", 'a', 'chat', ',', 'a', 'wiki', ',', 'a', 'link', 'to', 'the', 'source', 'code', 'and', 'some', 'other', 'info', 'on', 'there', '.']\n",
            "After Stemming: \n",
            "2\n",
            "Comment: I thought they just copy-paste everything from StackOverflow\n",
            "['I', 'thought', 'they', 'just', 'copy-paste', 'everything', 'from', 'StackOverflow']\n",
            "After Stemming: \n",
            "1\n",
            "[('Spam', 'NNP'), ('in', 'IN'), ('programming', 'VBG')]\n",
            "[('Most', 'JJS'), ('of', 'IN'), ('all', 'PDT'), ('these', 'DT'), ('hats', 'NNS'), ('are', 'VBP'), ('just', 'RB'), ('``', '``'), ('presh', 'JJ'), (\"'cos\", 'NN'), (\"''\", \"''\")]\n",
            "[('r/testflight', 'NN')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}